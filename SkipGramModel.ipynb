{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading in mutiple files\n",
    "\n",
    "import re\n",
    "\n",
    "text_infile = 'text_corpus.txt'\n",
    "\n",
    "def clean_text(s):\n",
    "    revals = re.compile('\\[[^\\]]*\\]|[^\\sA-Za-z\\-\\!\\.\\?/\\';:,]')\n",
    "    rewhitespace = re.compile('\\s+')\n",
    "    stripped = revals.sub('', s)\n",
    "    stripped = rewhitespace.sub(' ', stripped).lower() # replace whitespace with single space\n",
    "    return stripped\n",
    "\n",
    "with open(text_infile) as infile:\n",
    "    data = infile.read()\n",
    "    data = clean_text(data)\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in multiple files and consolidate text into a single list\n",
    "import os\n",
    "\n",
    "root_dir = 'Gutenberg/txt/' # root \n",
    "\n",
    "def get_size(path):\n",
    "        \"\"\"Function returns the size of a given file in megabytes.\"\"\"\n",
    "        size = os.path.getsize(path)\n",
    "        return size/1000000\n",
    "\n",
    "\n",
    "def find_files(rootdir, max_size):\n",
    "    \"\"\"Find all files in a given subdirectory and create\"\"\"\n",
    "    filesInTxt = []\n",
    "    for filename in os.listdir(rootdir):\n",
    "        fiSize = get_size(rootdir + filename)\n",
    "        if filename.endswith(\".txt\") and fiSize < max_size:\n",
    "            filesInTxt.append('Gutenberg/txt/' + filename)\n",
    "    return filesInTxt\n",
    "\n",
    "file_list = find_files('Gutenberg/txt/', 0.1)\n",
    "print(f\"Number of files processed to created corpus {len(file_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement text preprocessing functions with regex and spacy\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocessing_vectors(corpus):\n",
    "    \"\"\"Remove stopwords, punctuation, and then convert text back into string.\"\"\"\n",
    "    text = nlp(corpus)\n",
    "    text = [token.text for token in text if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(text)\n",
    "\n",
    "def write_out_corpus(file_list, file_name):\n",
    "    \"\"\"Clean all text data, then output it to a txt file.\"\"\"\n",
    "    outFile = open(file_name, 'w')\n",
    "    pattern = re.compile('\\[[^\\]]*\\]|[^\\w\\s\\.\\!\\?\\,\\'\\(\\)\\\"\\/\\;\\_]')\n",
    "    whitespace = re.compile('\\s+')\n",
    "    i = 0\n",
    "\n",
    "    for file in file_list:\n",
    "        if (i % 50 == 0):\n",
    "                logging.info(\"read {0} reviews\".format(i))\n",
    "        with open(file) as infile:\n",
    "            data = infile.read()\n",
    "            data = pattern.sub('', data)\n",
    "            data = whitespace.sub(' ', data).lower()\n",
    "            data = preprocessing_vectors(data) # remove stopwords & punctuation\n",
    "            outFile.write(data) # write out to gzip file\n",
    "            infile.close()\n",
    "        i = i + 1\n",
    "    outFile.close()\n",
    "\n",
    "write_out_corpus(file_list, 'outText.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement SkipGram Model with Python Class\n",
    "\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute e^x/sum(e^x) - softmax values for each score in x\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "class SkipGram(object):\n",
    "    def __init__(self): \n",
    "        self.N = 20 # Number of neurons in the hidden layer\n",
    "        self.X_train = [] # space for training data tokens\n",
    "        self.y_train = [] # list for probability outputs\n",
    "        self.window = 2 # range to the front and back of center word use\n",
    "        self.alpha = 0.001 # learning rate\n",
    "        self.words = [] # list of words relevant to our dat model\n",
    "        self.word_index = {} # dictionary for word and index pair\n",
    "\n",
    "    def initialize_model(self, V, data):\n",
    "        self.V = V\n",
    "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
    "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
    "           \n",
    "        self.words = data\n",
    "        for i in range(len(data)): \n",
    "            self.word_index[data[i]] = i\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        self.h = np.dot(self.W.T, X).reshape(self.N, 1)\n",
    "        self.u = np.dot(self.W1.T, self.h)\n",
    "        self.y = softmax(self.u)\n",
    "        return self.y\n",
    "\n",
    "    def backpropagate(self, x, t):\n",
    "        e = self.y - np.asarray(t).reshape(self.V, 1)\n",
    "        dLdW1 = np.dot(self.h,e.T) \n",
    "        X = np.array(x).reshape(self.V,1) \n",
    "        dLdW = np.dot(X, np.dot(self.W1,e).T) \n",
    "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
    "        self.W = self.W - self.alpha*dLdW\n",
    "    \n",
    "    def train(self, epochs):\n",
    "        for x in range(1, epochs):\n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)):\n",
    "                self.feed_forward(self.X_train[j])\n",
    "                self.backpropagate(self.X_train[j], self.y_train[j])\n",
    "                \n",
    "                C = 0\n",
    "                for m in range(self.V):\n",
    "                    if (self.y_train[j][m]):\n",
    "                        self.loss += -1*self.u[m][0] \n",
    "                        C += 1\n",
    "                self.loss += C * np.log(np.sum(np.exp(self.u)))\n",
    "            if x % 100 == 0:\n",
    "                print(\"epoch {} loss = {}\".format(x, self.loss))\n",
    "            self.alpha *= 1/((1+self.alpha*x))\n",
    "    \n",
    "    def predict(self, word, num_predictions):\n",
    "        if word in self.words:\n",
    "            index = self.word_index[word]\n",
    "            X = [0 for i in range(self.V)]\n",
    "            X[index] = 1\n",
    "            prediction = self.feed_forward(X)\n",
    "            output = {}\n",
    "            for i in range(self.V):\n",
    "                output[prediction[i][0]] = i\n",
    "            \n",
    "            top_words = []\n",
    "            for k in sorted(output, reverse=True):\n",
    "                top_words.append(self.words[output[k]])\n",
    "                if (len(top_words) >= num_predictions):\n",
    "                    break\n",
    "            return top_words\n",
    "        else:\n",
    "            print(\"Word not found in dictionary.\")\n",
    "            \n",
    "    \n",
    "def buildDictionary(sentences, skip_gram):\n",
    "    data = {}\n",
    "    # Count all distinct words\n",
    "    for sentence in sentences:\n",
    "        for word in sentence: \n",
    "            if word not in data:\n",
    "                data[word] = 1\n",
    "            else:\n",
    "                data[word] += 1\n",
    "    V = len(data)\n",
    "    # prepare list for one hot encoding\n",
    "    data = sorted(list(data.keys()))\n",
    "    vocabulary = {}\n",
    "    for i in range(len(data)):\n",
    "        vocabulary[data[i]] = i\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)):\n",
    "            center_word = [0 for x in range(V)]\n",
    "            center_word[vocabulary[sentence[i]]] = 1\n",
    "            context = [0 for x in range(V)]\n",
    "            \n",
    "            for j in range(i-skip_gram.window, i+skip_gram.window):\n",
    "                if i != j and j >= 0 and j < len(sentence):\n",
    "                    context[vocabulary[sentence[j]]] += 1\n",
    "            skip_gram.X_train.append(center_word)\n",
    "            skip_gram.y_train.append(context)\n",
    "    skip_gram.initialize_model(V, data)\n",
    "    \n",
    "    return skip_gram.X_train, skip_gram.y_train\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "SkipModel = SkipGram()\n",
    "pre_processed_text = preprocessing_vectors(test_text)\n",
    "prepare_data(pre_processed_text, SkipModel)\n",
    "SkipModel.train(epochs)\n",
    "print(SkipModel.predict('computing', 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try gensim implementation of Word2Vec using separate corpus\n",
    "\n",
    "import gensim\n",
    "import gzip\n",
    "\n",
    "data_file=\"text.txt.gz\"\n",
    "\n",
    "def read_input(input_file):\n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "            # Remove accents and convert to unicode\n",
    "            yield gensim.utils.simple_preprocess(line) \n",
    "\n",
    "documents = list(read_input(data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Synonyms for dirty: \\n {model.wv.most_similar(positive=\"dirty\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Synonyms for happy: \\n {model.wv.most_similar(positive=\"happy\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = \"hate\"\n",
    "model.wv.most_similar(positive=w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate above model on wordsim353 dataset\n",
    "# For more on wordsim353 see: http://alfonseca.org/eng/research/wordsim353.html\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/ezramacdonald/Downloads/wordsim353/combined.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Human mean score\n",
    "df['Human (mean)'] = (df['Human (mean)'] - df['Human (mean)'].min())/(df['Human (mean)'].max() - df['Human (mean)'].min())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute score of similarilty between two words based on our model\n",
    "\n",
    "def similarity_score(x, y):\n",
    "    if str(x) not in model.wv.vocab or str(y) not in model.wv.vocab:\n",
    "        return 0\n",
    "    return model.wv.similarity(str(x), str(y))\n",
    "\n",
    "df['Score'] = df.apply(lambda x: similarity_score(x['Word 1'], x['Word 2']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false = len((df[(df['Score'] <= 0.5) & (df['Human (mean)'] <= 0.5)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_true = len((df[(0.5 <= df['Score']) & (0.5 <= df['Human (mean)'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results of model based on training for ~30 minutes on Amazon Fine Food Reviews dataset.\n",
    "print(f\"Model effectiveness based on wordsim353 evaluation: {(true_true+true_false)/len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
